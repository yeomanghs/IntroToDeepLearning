{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Final Project\n",
    "\n",
    "In this final project you will define and train an image-to-caption model, that can produce descriptions for real world images!\n",
    "\n",
    "<img src=\"images/encoder_decoder.png\" style=\"width:70%\">\n",
    "\n",
    "Model architecture: CNN encoder and RNN decoder. \n",
    "(https://research.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "import download_utils\n",
    "\n",
    "#version: 1.15\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "L = keras.layers\n",
    "K = keras.backend\n",
    "import utils\n",
    "import time\n",
    "import zipfile\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "from random import choice\n",
    "import grading_utils\n",
    "import os\n",
    "from keras_utils import reset_tf_session\n",
    "import tqdm_utils\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\figohjs\\Desktop\\IntroToDeepLearning\\Week6\\weights_10\n"
     ]
    }
   ],
   "source": [
    "# Leave USE_GOOGLE_DRIVE = False if you're running locally!\n",
    "# We recommend to set USE_GOOGLE_DRIVE = True in Google Colab!\n",
    "# If set to True, we will mount Google Drive, so that you can restore your checkpoint \n",
    "# and continue trainig even if your previous Colab session dies.\n",
    "# If set to True, follow on-screen instructions to access Google Drive (you must have a Google account).\n",
    "USE_GOOGLE_DRIVE = False\n",
    "\n",
    "def mount_google_drive():\n",
    "    from google.colab import drive\n",
    "    mount_directory = \"/content/gdrive\"\n",
    "    drive.mount(mount_directory)\n",
    "    drive_root = mount_directory + \"/\" + list(filter(lambda x: x[0] != '.', os.listdir(mount_directory)))[0] + \"/colab\"\n",
    "    return drive_root\n",
    "\n",
    "CHECKPOINT_ROOT = \"\"\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    CHECKPOINT_ROOT = mount_google_drive() + \"/\"\n",
    "\n",
    "def get_checkpoint_path(epoch=None):\n",
    "    if epoch is None:\n",
    "        return os.path.abspath(CHECKPOINT_ROOT + \"weights\")\n",
    "    else:\n",
    "        return os.path.abspath(CHECKPOINT_ROOT + \"weights_{}\".format(epoch))\n",
    "      \n",
    "# example of checkpoint dir\n",
    "print(get_checkpoint_path(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in your Coursera token and email\n",
    "To successfully submit your answers to our grader, please fill in your Coursera submission token and email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = grading.Grader(assignment_key=\"NEDBg6CgEee8nQ6uE8a7OA\", \n",
    "                        all_parts=[\"19Wpv\", \"uJh73\", \"yiJkt\", \"rbpnH\", \"E2OIL\", \"YJR7z\"])\n",
    "\n",
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"PDnLOZNkKAsOpp5i\"### YOUR TOKEN HERE\n",
    "COURSERA_EMAIL = \"yeoman_ghs@hotmail.com\"### YOUR EMAIL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image features\n",
    "\n",
    "We will use pre-trained InceptionV3 model for CNN encoder (https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html) and extract its last hidden layer as an embedding:\n",
    "\n",
    "<img src=\"images/inceptionv3.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image features\n",
    "\n",
    "We will use pre-trained InceptionV3 model for CNN encoder (https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html) and extract its last hidden layer as an embedding:\n",
    "\n",
    "<img src=\"images/inceptionv3.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 299\n",
    "\n",
    "# we take the last hidden layer of IncetionV3 as an image embedding\n",
    "def get_cnn_encoder():\n",
    "    K.set_learning_phase(False)\n",
    "    model = keras.applications.InceptionV3(include_top=False)\n",
    "    preprocess_for_model = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    model = keras.models.Model(model.inputs, keras.layers.GlobalAveragePooling2D()(model.output))\n",
    "    return model, preprocess_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82783, 2048) 82783\n",
      "(40504, 2048) 40504\n"
     ]
    }
   ],
   "source": [
    "# load prepared embeddings\n",
    "train_img_embeds = utils.read_pickle(\"train_img_embeds.pickle\")\n",
    "train_img_fns = utils.read_pickle(\"train_img_fns.pickle\")\n",
    "val_img_embeds = utils.read_pickle(\"val_img_embeds.pickle\")\n",
    "val_img_fns = utils.read_pickle(\"val_img_fns.pickle\")\n",
    "# check shapes\n",
    "print(train_img_embeds.shape, len(train_img_fns))\n",
    "print(val_img_embeds.shape, len(val_img_fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract captions for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82783 82783\n",
      "40504 40504\n"
     ]
    }
   ],
   "source": [
    "# extract captions from zip\n",
    "def get_captions_for_fns(fns, zip_fn, zip_json_path):\n",
    "    zf = zipfile.ZipFile(zip_fn)\n",
    "    j = json.loads(zf.read(zip_json_path).decode(\"utf8\"))\n",
    "    id_to_fn = {img[\"id\"]: img[\"file_name\"] for img in j[\"images\"]}\n",
    "    fn_to_caps = defaultdict(list)\n",
    "    for cap in j['annotations']:\n",
    "        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])\n",
    "    fn_to_caps = dict(fn_to_caps)\n",
    "    return list(map(lambda x: fn_to_caps[x], fns))\n",
    "    \n",
    "train_captions = get_captions_for_fns(train_img_fns, \"data/captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_train2014.json\")\n",
    "\n",
    "val_captions = get_captions_for_fns(val_img_fns, \"data/captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_val2014.json\")\n",
    "\n",
    "# check shape\n",
    "print(len(train_img_fns), len(train_captions))\n",
    "print(len(val_img_fns), len(val_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare captions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = \"#PAD#\"\n",
    "UNK = \"#UNK#\"\n",
    "START = \"#START#\"\n",
    "END = \"#END#\"\n",
    "\n",
    "# split sentence into tokens (split into lowercased words)\n",
    "def split_sentence(sentence):\n",
    "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
    "\n",
    "#source:https://thispointer.com/python-filter-a-dictionary-by-conditions-on-keys-or-values/\n",
    "def filterTheDict(dictObj, callback):\n",
    "    newDict = dict()\n",
    "    # Iterate over all the items in dictionary\n",
    "    for (key, value) in dictObj.items():\n",
    "        # Check if item satisfies the given condition then add to new dict\n",
    "        if callback((key, value)):\n",
    "            newDict[key] = value\n",
    "    return newDict\n",
    "\n",
    "def generate_vocabulary(train_captions):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur 5 times or more, \n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    Use `split_sentence` function to split sentence into tokens.\n",
    "    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), \n",
    "        START (start of sentence) and END (end of sentence) tokens into the vocabulary.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    #tokenize each sentence in each paragraph: [paragraph1, paragraph2] = [[sentence1, sentence2], [sentence 1, sentence2]]\n",
    "    wordsList = [split_sentence(sentence) for paragraph in train_captions for sentence in paragraph]\n",
    "    flattenWordsList = list(itertools.chain(*wordsList))\n",
    "    \n",
    "    #take count of each token in flattenWordsList in form of dict\n",
    "    wordCount_Dict = defaultdict(int)\n",
    "    for word in flattenWordsList:\n",
    "        wordCount_Dict[word] += 1\n",
    "    filterDict = filterTheDict(wordCount_Dict, lambda elem : elem[1] >= 5)\n",
    "    \n",
    "    #store filtered tokens and additional one into vocab\n",
    "    vocab = list(filterDict.keys()) + [PAD, UNK, START, END]\n",
    "    \n",
    "    return {token: index for index, token in enumerate(sorted(vocab))}\n",
    "    \n",
    "def caption_tokens_to_indices(captions, vocabDict):\n",
    "    \"\"\"\n",
    "    `captions` argument is an array of arrays:\n",
    "    [\n",
    "        [\n",
    "            \"image1 caption1\",\n",
    "            \"image1 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        [\n",
    "            \"image2 caption1\",\n",
    "            \"image2 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    Use `split_sentence` function to split sentence into tokens.\n",
    "    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
    "    Add START and END tokens to start and end of each sentence respectively.\n",
    "    For the example above you should produce the following:\n",
    "    [\n",
    "        [\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    res = []\n",
    "    #Use `split_sentence` function to split sentence into tokens.\n",
    "    for paragraph in captions:\n",
    "        tempParagraphList = []\n",
    "        for sentence in paragraph:\n",
    "            tokenizedSentence = split_sentence(sentence)\n",
    "            #Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
    "            #Add START and END tokens to start and end of each sentence respectively.\n",
    "            tempList =  [vocabDict[START]] + \\\n",
    "                        [vocabDict[token] if token in vocabDict.keys() else vocabDict[UNK] \n",
    "                         for token in tokenizedSentence] + \\\n",
    "                        [vocabDict[END]]\n",
    "            tempParagraphList.append(tempList)\n",
    "        res.append(tempParagraphList)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vocabulary\n",
    "vocab = generate_vocabulary(train_captions)\n",
    "vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
    "\n",
    "# replace tokens with indices\n",
    "train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)\n",
    "val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "# Vocabulary creation\n",
    "grader.set_answer(\"19Wpv\", grading_utils.test_vocab(vocab, PAD, UNK, START, END))\n",
    "# Captions indexing\n",
    "grader.set_answer(\"uJh73\", grading_utils.test_captions_indexing(train_captions_indexed, vocab, UNK))\n",
    "# Captions batching\n",
    "grader.set_answer(\"yiJkt\", grading_utils.test_captions_batching(batch_captions_to_matrix))\n",
    "\n",
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captions have different length, but we need to batch them, that's why we will add PAD tokens so that all sentences have an equal length. \n",
    "\n",
    "We will crunch LSTM through all the tokens, but we will ignore padding tokens during loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this during training\n",
    "def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):\n",
    "    \"\"\"\n",
    "    `batch_captions` is an array of arrays:\n",
    "    [\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        ...\n",
    "    ]\n",
    "    Put vocabulary indexed captions into np.array of shape (len(batch_captions), columns),\n",
    "        where \"columns\" is max(map(len, batch_captions)) when max_len is None\n",
    "        and \"columns\" = min(max_len, max(map(len, batch_captions))) otherwise.\n",
    "    Add padding with pad_idx where necessary.\n",
    "    Input example: [[1, 2, 3], [4, 5]]\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None\n",
    "    Output example: np.array([[1, 2], [4, 5]]) if max_len=2\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100\n",
    "    Try to use numpy, we need this function to be fast!\n",
    "    \"\"\"\n",
    "    ###YOUR CODE HERE###\n",
    "    #if maxLen is None, take max of len of any sentences; else, take max between maxLen and len of any sentences\n",
    "    colLength = [min(max_len, max(map(len, batch_captions))) if max_len else max(map(len, batch_captions))][0] \n",
    "\n",
    "    #initialize a matrix with dim num of sentences X col\n",
    "    matrix = np.zeros(shape=(len(batch_captions), colLength))\n",
    "    \n",
    "    #loop through every captions, standardize number of col == colLength\n",
    "    for no, caption in enumerate(batch_captions):\n",
    "        #if colLength is more than length of that caption\n",
    "        if len(caption) < colLength:\n",
    "            #add more pad ind to end of caption\n",
    "            matrix[no] = np.pad(caption, (0, colLength - len(caption)), 'constant', constant_values = pad_idx)\n",
    "        #if colLength is less than length of that caption\n",
    "        elif len(caption) > colLength:\n",
    "            #truncate caption til colLength\n",
    "            matrix[no] = np.array(caption[:colLength])\n",
    "        #if colLength is equal to length of that caption\n",
    "        else:\n",
    "            #convert caption into np array\n",
    "            matrix[no] = np.array(caption)\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since our problem is to generate image captions, RNN text generator should be conditioned on image. The idea is to use image features as an initial state for RNN instead of zeros. \n",
    "\n",
    "Remember that you should transform image feature vector to RNN hidden state size by fully-connected layer and then pass it to RNN.\n",
    "\n",
    "During training we will feed ground truth tokens into the lstm to get predictions of next tokens. \n",
    "\n",
    "Notice that we don't need to feed last token (END) as input (http://cs.stanford.edu/people/karpathy/):\n",
    "\n",
    "<img src=\"images/encoder_decoder_explained.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\figohjs\\Desktop\\IntroToDeepLearning\\Week6\\keras_utils.py:68: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\figohjs\\Desktop\\IntroToDeepLearning\\Week6\\keras_utils.py:75: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Users\\figohjs\\Desktop\\IntroToDeepLearning\\Week6\\keras_utils.py:77: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\users\\figohjs\\documents\\tf1testenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-12-bf9ca1cf28a0>:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From d:\\users\\figohjs\\documents\\tf1testenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-12-bf9ca1cf28a0>:57: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From d:\\users\\figohjs\\documents\\tf1testenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From d:\\users\\figohjs\\documents\\tf1testenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "IMG_EMBED_SIZE = train_img_embeds.shape[1]\n",
    "IMG_EMBED_BOTTLENECK = 120\n",
    "WORD_EMBED_SIZE = 100\n",
    "LSTM_UNITS = 300\n",
    "LOGIT_BOTTLENECK = 120\n",
    "pad_idx = vocab[PAD]\n",
    "\n",
    "# remember to reset your graph if you want to start building it from scratch!\n",
    "s = reset_tf_session()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "class decoder:\n",
    "    # [batch_size, IMG_EMBED_SIZE] of CNN image features\n",
    "    img_embeds = tf.placeholder('float32', [None, IMG_EMBED_SIZE])\n",
    "    # [batch_size, time steps] of word ids\n",
    "    sentences = tf.placeholder('int32', [None, None])\n",
    "    \n",
    "    # we use bottleneck here to reduce the number of parameters\n",
    "    # image embedding -> bottleneck\n",
    "    img_embed_to_bottleneck = L.Dense(IMG_EMBED_BOTTLENECK, \n",
    "                                      input_shape=(None, IMG_EMBED_SIZE), \n",
    "                                      activation='elu')\n",
    "    # image embedding bottleneck -> lstm initial state\n",
    "    img_embed_bottleneck_to_h0 = L.Dense(LSTM_UNITS,\n",
    "                                         input_shape=(None, IMG_EMBED_BOTTLENECK),\n",
    "                                         activation='elu')\n",
    "    # word -> embedding\n",
    "    word_embed = L.Embedding(len(vocab), WORD_EMBED_SIZE)\n",
    "    # lstm cell (from tensorflow)\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(LSTM_UNITS)\n",
    "    \n",
    "    # we use bottleneck here to reduce model complexity\n",
    "    # lstm output -> logits bottleneck\n",
    "#     token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, \n",
    "#                                       input_shape=(None, LSTM_UNITS),\n",
    "#                                       activation=\"elu\")\n",
    "    token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, activation=\"elu\")\n",
    "    # logits bottleneck -> logits for next token prediction\n",
    "#     token_logits = L.Dense(len(vocab),\n",
    "#                            input_shape=(None, LOGIT_BOTTLENECK))\n",
    "    token_logits = L.Dense(len(vocab))\n",
    "    \n",
    "    # initial lstm cell state of shape (None, LSTM_UNITS),\n",
    "    # we need to condition it on `img_embeds` placeholder.\n",
    "    c0 = h0 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds)) ### YOUR CODE HERE ###\n",
    "\n",
    "    # embed all tokens but the last for lstm input,\n",
    "    # remember that L.Embedding is callable,\n",
    "    # use `sentences` placeholder as input.\n",
    "    word_embeds = word_embed(sentences[:,:-1]) ### YOUR CODE HERE ###\n",
    "    \n",
    "    # during training we use ground truth tokens `word_embeds` as context for next token prediction.\n",
    "    # that means that we know all the inputs for our lstm and can get \n",
    "    # all the hidden states with one tensorflow operation (tf.nn.dynamic_rnn).\n",
    "    # `hidden_states` has a shape of [batch_size, time steps, LSTM_UNITS].\n",
    "    hidden_states, _ = tf.nn.dynamic_rnn(lstm, word_embeds,\n",
    "                                         initial_state=tf.nn.rnn_cell.LSTMStateTuple(c0, h0))\n",
    "\n",
    "    # now we need to calculate token logits for all the hidden states\n",
    "    \n",
    "    # first, we reshape `hidden_states` to [-1, LSTM_UNITS]\n",
    "    flat_hidden_states = tf.reshape(hidden_states, [-1,LSTM_UNITS]) ### YOUR CODE HERE ###\n",
    "\n",
    "    # then, we calculate logits for next tokens using `token_logits_bottleneck` and `token_logits` layers\n",
    "    flat_token_logits = token_logits(token_logits_bottleneck(flat_hidden_states)) ### YOUR CODE HERE ###\n",
    "    \n",
    "    # then, we flatten the ground truth token ids.\n",
    "    # remember, that we predict next tokens for each time step,\n",
    "    # use `sentences` placeholder.\n",
    "    flat_ground_truth = tf.reshape(sentences[:, 1:], [-1]) ### YOUR CODE HERE ###\n",
    "\n",
    "    # we need to know where we have real tokens (not padding) in `flat_ground_truth`,\n",
    "    # we don't want to propagate the loss for padded output tokens,\n",
    "    # fill `flat_loss_mask` with 1.0 for real tokens (not pad_idx) and 0.0 otherwise.\n",
    "    #flat_loss_mask = tf.not_equal(flat_ground_truth, pad_idx) ### YOUR CODE HERE ###\n",
    "    flat_loss_mask = tf.cast(tf.not_equal(flat_ground_truth,pad_idx), tf.float32) \n",
    "\n",
    "    # compute cross-entropy between `flat_ground_truth` and `flat_token_logits` predicted by lstm\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=flat_ground_truth, \n",
    "        logits=flat_token_logits\n",
    "    )\n",
    "\n",
    "    # compute average `xent` over tokens with nonzero `flat_loss_mask`.\n",
    "    # we don't want to account misclassification of PAD tokens, because that doesn't make sense,\n",
    "    # we have PAD tokens for batching purposes only!\n",
    "    #loss = tf.reduce_mean(tf.multiply(xent,  tf.cast(flat_loss_mask, tf.float32))) ### YOUR CODE HERE ###\n",
    "    loss =  tf.reduce_sum(tf.multiply(xent, flat_loss_mask)) / tf.reduce_sum(flat_loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer operation to minimize the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_step = optimizer.minimize(decoder.loss)\n",
    "\n",
    "# will be used to save/load network weights.\n",
    "# you need to reset your default graph and define it in the same way to be able to load the saved weights!\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# intialize all variables\n",
    "s.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "# Decoder shapes test\n",
    "grader.set_answer(\"rbpnH\", grading_utils.test_decoder_shapes(decoder, IMG_EMBED_SIZE, vocab, s))\n",
    "# Decoder random loss test\n",
    "grader.set_answer(\"E2OIL\", grading_utils.test_random_decoder_loss(decoder, IMG_EMBED_SIZE, vocab, s))\n",
    "\n",
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Evaluate train and validation metrics through training and log them. Ensure that loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions_indexed = np.array(train_captions_indexed)\n",
    "val_captions_indexed = np.array(val_captions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batch via random sampling of images and captions for them,\n",
    "# we use `max_len` parameter to control the length of the captions (truncating long captions)\n",
    "def generate_batch(images_embeddings, indexed_captions, batch_size, max_len=None):\n",
    "    \"\"\"\n",
    "    `images_embeddings` is a np.array of shape [number of images, IMG_EMBED_SIZE].\n",
    "    `indexed_captions` holds 5 vocabulary indexed captions for each image:\n",
    "    [\n",
    "        [\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    Generate a random batch of size `batch_size`.\n",
    "    Take random images and choose one random caption for each image.\n",
    "    Remember to use `batch_captions_to_matrix` for padding and respect `max_len` parameter.\n",
    "    Return feed dict {decoder.img_embeds: ..., decoder.sentences: ...}.\n",
    "    \"\"\"\n",
    "    idx = np.random.choice(images_embeddings.shape[0], batch_size)\n",
    "    batch_image_embeddings = images_embeddings[idx] ### YOUR CODE HERE ###\n",
    "    \n",
    "    cap_idx = np.random.choice(5, batch_size)\n",
    "    \n",
    "    batch_captions = []\n",
    "    for i in range(batch_size):\n",
    "        sample = indexed_captions[idx[i]][cap_idx[i]]\n",
    "        batch_captions.append(sample)\n",
    "    \n",
    "    \n",
    "    # TODO pad\n",
    "    batch_captions_matrix = batch_captions_to_matrix(batch_captions, 0, max_len)\n",
    "    \n",
    "    return {decoder.img_embeds: batch_image_embeddings, \n",
    "            decoder.sentences: batch_captions_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#n_epochs = 12\n",
    "n_epochs = 5\n",
    "n_batches_per_epoch = 1000\n",
    "n_validation_batches = 100  # how many batches are used for validation after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load trained weights here\n",
    "# uncomment the next line if you need to load weights\n",
    "#saver.restore(s, get_checkpoint_path(epoch=4))\n",
    "saver.restore(s, os.path.abspath(\"weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e2f0c7f6374dd48c02d4236c941e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, train loss: 1.9471137914657592, val loss: 1.8649835979938507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b688089a84f7ca0df57f49c719f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, train loss: 1.8208029637336731, val loss: 1.8201050341129303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e0db6178f4483fa37ada1feddf9913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, train loss: 1.7499373817443848, val loss: 1.7735350441932678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c991bd751c1e49459c5802f13b8e143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, train loss: 1.7078919240236283, val loss: 1.7291428554058075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d547cc87584f7aac7921392088651b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4, train loss: 1.6641145417690277, val loss: 1.692144502401352\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# actual training loop\n",
    "MAX_LEN = 20  # truncate long captions to speed up training\n",
    "\n",
    "# to make training reproducible\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    pbar = tqdm_utils.tqdm_notebook_failsafe(range(n_batches_per_epoch))\n",
    "    counter = 0\n",
    "    for _ in pbar:\n",
    "        train_loss += s.run([decoder.loss, train_step], \n",
    "                            generate_batch(train_img_embeds, \n",
    "                                           train_captions_indexed, \n",
    "                                           batch_size, \n",
    "                                           MAX_LEN))[0]\n",
    "        counter += 1\n",
    "        pbar.set_description(\"Training loss: %f\" % (train_loss / counter))\n",
    "        \n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss = 0\n",
    "    for _ in range(n_validation_batches):\n",
    "        val_loss += s.run(decoder.loss, generate_batch(val_img_embeds,\n",
    "                                                       val_captions_indexed, \n",
    "                                                       batch_size, \n",
    "                                                       MAX_LEN))\n",
    "    val_loss /= n_validation_batches\n",
    "    \n",
    "    print('Epoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "    # save weights after finishing epoch\n",
    "    saver.save(s, get_checkpoint_path(epoch))\n",
    "    \n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de63465c480467fb7d09d9cd13bf90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valLoss = grading_utils.test_validation_loss(\n",
    "        decoder, s, generate_batch, val_img_embeds, val_captions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7759044607877732\n"
     ]
    }
   ],
   "source": [
    "print(valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "# Validation loss\n",
    "\n",
    "# valLoss = grader.set_answer(\"YJR7z\", grading_utils.test_validation_loss(\n",
    "#     decoder, s, generate_batch, val_img_embeds, val_captions_indexed))\n",
    "grader.set_answer(\"YJR7z\", valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You used an invalid email or your token may have expired. Please make sure you have entered all fields correctly. Try generating a new token if the issue still persists.\n"
     ]
    }
   ],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save last graph weights to file!\n",
    "saver.save(s, get_checkpoint_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying model\n",
    "\n",
    "Here we construct a graph for our final model.\n",
    "\n",
    "It will work as follows:\n",
    "- take an image as an input and embed it\n",
    "- condition lstm on that embedding\n",
    "- predict the next token given a START input token\n",
    "- use predicted token as an input at next time step\n",
    "- iterate until you predict an END token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\Users\\figohjs\\Desktop\\IntroToDeepLearning\\Week6\\weights\n"
     ]
    }
   ],
   "source": [
    "class final_model:\n",
    "    # CNN encoder\n",
    "    encoder, preprocess_for_model = get_cnn_encoder()\n",
    "    saver.restore(s, get_checkpoint_path())  # keras applications corrupt our graph, so we restore trained weights\n",
    "    \n",
    "    # containers for current lstm state\n",
    "    lstm_c = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"cell\")\n",
    "    lstm_h = tf.Variable(tf.zeros([1, LSTM_UNITS]), name=\"hidden\")\n",
    "\n",
    "    # input images\n",
    "    input_images = tf.placeholder('float32', [1, IMG_SIZE, IMG_SIZE, 3], name='images')\n",
    "\n",
    "    # get image embeddings\n",
    "    img_embeds = encoder(input_images)\n",
    "\n",
    "    # initialize lstm state conditioned on image\n",
    "    init_c = init_h = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\n",
    "    init_lstm = tf.assign(lstm_c, init_c), tf.assign(lstm_h, init_h)\n",
    "    \n",
    "    # current word index\n",
    "    current_word = tf.placeholder('int32', [1], name='current_input')\n",
    "\n",
    "    # embedding for current word\n",
    "    word_embed = decoder.word_embed(current_word)\n",
    "\n",
    "    # apply lstm cell, get new lstm states\n",
    "    new_c, new_h = decoder.lstm(word_embed, tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h))[1]\n",
    "\n",
    "    # compute logits for next token\n",
    "    new_logits = decoder.token_logits(decoder.token_logits_bottleneck(new_h))\n",
    "    # compute probabilities for next token\n",
    "    new_probs = tf.nn.softmax(new_logits)\n",
    "\n",
    "    # `one_step` outputs probabilities of next token and updates lstm hidden state\n",
    "    one_step = new_probs, tf.assign(lstm_c, new_c), tf.assign(lstm_h, new_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at how temperature works for probability distributions\n",
    "# for high temperature we have more uniform distribution\n",
    "_ = np.array([0.5, 0.4, 0.1])\n",
    "for t in [0.01, 0.1, 1, 10, 100]:\n",
    "    print(\" \".join(map(str, _**(1/t) / np.sum(_**(1/t)))), \"with temperature\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an actual prediction loop\n",
    "def generate_caption(image, t=1, sample=False, max_len=20):\n",
    "    \"\"\"\n",
    "    Generate caption for given image.\n",
    "    if `sample` is True, we will sample next token from predicted probability distribution.\n",
    "    `t` is a temperature during that sampling,\n",
    "        higher `t` causes more uniform-like distribution = more chaos.\n",
    "    \"\"\"\n",
    "    # condition lstm on the image\n",
    "    s.run(final_model.init_lstm, \n",
    "          {final_model.input_images: [image]})\n",
    "    \n",
    "    # current caption\n",
    "    # start with only START token\n",
    "    caption = [vocab[START]]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        next_word_probs = s.run(final_model.one_step, \n",
    "                                {final_model.current_word: [caption[-1]]})[0]\n",
    "        next_word_probs = next_word_probs.ravel()\n",
    "        \n",
    "        # apply temperature\n",
    "        next_word_probs = next_word_probs**(1/t) / np.sum(next_word_probs**(1/t))\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(range(len(vocab)), p=next_word_probs)\n",
    "        else:\n",
    "            next_word = np.argmax(next_word_probs)\n",
    "\n",
    "        caption.append(next_word)\n",
    "        if next_word == vocab[END]:\n",
    "            break\n",
    "       \n",
    "    return list(map(vocab_inverse.get, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at validation prediction example\n",
    "def apply_model_to_image_raw_bytes(raw):\n",
    "    img = utils.decode_image_from_buf(raw)\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.grid('off')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)\n",
    "    print(' '.join(generate_caption(img)[1:-1]))\n",
    "    plt.show()\n",
    "\n",
    "def show_valid_example(val_img_fns, example_idx=0):\n",
    "    zf = zipfile.ZipFile(\"data/val2014_sample.zip\")\n",
    "    all_files = set(val_img_fns)\n",
    "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
    "    example = found_files[example_idx]\n",
    "    apply_model_to_image_raw_bytes(zf.read(example))\n",
    "    \n",
    "show_valid_example(val_img_fns, example_idx=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample more images from validation\n",
    "for idx in np.random.choice(range(len(zipfile.ZipFile(\"data/val2014_sample.zip\").filelist) - 1), 10):\n",
    "    show_valid_example(val_img_fns, example_idx=idx)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tf1TestEnv",
   "language": "python",
   "name": "tf1testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
